/* BASIC NEURAL NET IN SCALA
by M. Kabir, kabir.naim@gmail.com
Drawing upon many fantasic resources, such as:
Python Basic Neural Net: http://iamtrask.github.io/2015/07/12/basic-python-network/
Learning Scala By Example: http://www.scala-lang.org/docu/files/ScalaByExample.pdf
Linear Algebra In Scala: http://www.scalaclass.com/book/export/html/1
*/

def readCSVtoDouble(csvfile: String) : Array[Array[Double]] = {
	io.Source.fromFile(csvfile)
    		.getLines()
    		.map(_.split(",").map(_.trim.toDouble))
    		.toArray	 //convert to Array. This entire thing is returned, scala doesnt need you to specify
}


def dot(mat1 : Array[Double], mat2: Array[Double]) : Double = {
		mat1
		.zip(mat2) //zip puts corresponding Array indexes into tuples. Super convenient
		.map{t:(Double,Double) => t._1*t._2} //Map goes inside the tuples set up by .zip and applies the * function
		.reduceLeft(_+_)
}

def piecewise(mat1 : Array[Double], mat2: Array[Double]) : Array[Double] = {
		mat1
		.zip(mat2) //zip puts corresponding Array indexes into tuples. Super convenient
		.map{t:(Double,Double) => t._1*t._2} //Map goes inside the tuples set up by .zip and applies the * function
}

def piecewiseAdd(mat1 : Array[Double], mat2: Array[Double]) : Array[Double] = {
		mat1
		.zip(mat2) //zip puts corresponding Array indexes into tuples. Super convenient
		.map{t:(Double,Double) => t._1+t._2} //Map goes inside the tuples set up by .zip and applies the * function
}

def T(matrix : Array[Array[Double]]) : Array[Array[Double]] = {
	var cutmatrix = matrix; //Try finding a more scalaish way to do this. Recursive function maybe? Loops don't seem to be the right style
	var outputmatrix = Array.fill[Array[Double]](matrix(0).length)(Array.fill[Double](matrix.length)(0)) //Initialize an output 'matrix'
	for(c <- 0 to outputmatrix.length-1){
		outputmatrix(c) = cutmatrix.map(_.head)
		cutmatrix = cutmatrix.map(_.tail)}
	outputmatrix
}

def matmultiply(mat1 : Array[Array[Double]], mat2: Array[Array[Double]]) : Array[Array[Double]] = {
	var Tmat2 = T(mat2);
	var outputM = Array.fill[Array[Double]](mat1.length)(Array.fill[Double](Tmat2.length)(0))
	for(c <- 0 to mat1.length -1){
		for(cc <- 0 to Tmat2.length - 1){
			outputM(c)(cc) = dot(mat1(c),Tmat2(cc))
		}
	}
	outputM
}

def sigmoid(x: Double, derivative: Boolean) : Double = {
	if(derivative){Math.exp(x)/((Math.exp(x) + 1)*(Math.exp(x) + 1))}
	else{1/(1+Math.exp(-x))}
}

val rand = scala.util.Random
def NeuralNetTrainer(filename: String, iterations: Integer) : (Array[Array[Double]], Array[Double]) = {

	var fulldata = readCSVtoDouble(filename); //Reading in my toy file, generated by my .py script
	var targets = fulldata.map{_.reverse}.map{_.head}; //Getting targets, which are the last column of CSV
	var features = fulldata.map{_.reverse}.map{_.tail}.map{_.reverse}; //Getting features, which are the first N columns

	// Here I'm generating random weights for my two 'synaptic layers'
	var layer1 = Array.fill[Array[Double]](features(0).length)(Array.fill[Double](features.length)(2*rand.nextDouble-1)); 
	var layer2 = Array.fill[Double](targets.length)(2*rand.nextDouble-1); //This layer will be applied to outputs obtained from layer1

	for(run <- 0 to iterations-1){
		//Weight applications at each layer, feeding forward to the next layers.
		var out1 = matmultiply(features, layer1).map{_.map{x=> sigmoid(x,false)}}; //Each prediction is fed through the sigmoid function to return a value between 0 and 1
		var out2 = matmultiply(out1, T(Array(layer2))).map{_.map{x=> sigmoid(x,false)}}; //These map onto our actual predictions

		//Final layer prediction error
		var error2 = targets.zip(T(out2)(0)).map{t: (Double,Double) => t._1 - t._2}; 

		//BACK-PROPAGATION TIME!

		//Elementwise multiplication of error and 'confidence' of a weight
		var d_layer2 = Array(T(out2)(0).map{x=> sigmoid(x,true)}.zip(error2).map{t:(Double,Double) => t._1*t._2}); //Encapsulating in array so it can work with my... admittedly poorly formulated matmultiply function

		//Generating errors for each weight in layer1. The logic here is that the 'confidence'-based deltas should also be applied on layer1, on each weight element according to what it contributed (which are layer2 weights)
		var error1 = matmultiply(T(d_layer2),Array(layer2)); 
		
		//These errors are then ALSO confidence-weighted to form the delta-values we'll use to change weights!
		var d_layer1 = out1.map{_.map{x=> sigmoid(x,true)}}
				.zip(error1) //This puts the out1 arrays and error1 arrays in a tuple next to each other
				.map{t:(Array[Double],Array[Double]) => piecewise(t._1,t._2)}; //this runs my piecewise multiplier on the two vectors in the tuple

		//CHANGING WEIGHTS!
		layer2 = layer2.zip(d_layer2(0)).map{t:(Double,Double) => t._1+t._2};
		
		layer1 = layer1.zip(d_layer1).map{t:(Array[Double],Array[Double]) => piecewiseAdd(t._1,t._2)};
	}
	(layer1,layer2)
}


def NeuralNetPredictor(features:Array[Array[Double]], trainedlayers: (Array[Array[Double]], Array[Double])) : Array[Array[Int]] = {
	var layer1 = trainedlayers._1;
	var layer2 = trainedlayers._2;

	//Propagate data through layers, just like old times
	var out1 = matmultiply(features, layer1).map{_.map{x=> sigmoid(x,false)}};
	var out2 = matmultiply(out1, T(Array(layer2))).map{_.map{x=> sigmoid(x,false)}};

	//As a final step I just want to threshold and offer clean binary classifation
	var threshold = 0.5;
	var prediction = out2.map{_.map{x=> x > 0.5}.map{x:Boolean => if(x) 1 else 0}};
	
	prediction
}
