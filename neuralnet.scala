var r1 = Array(0.0,1.0,2.0,3.0,4.0);
var r2 = Array(5.0,4.0,3.0,2.0,1.0);
var r3 = Array(9.0,8.0,7.0,6.0,5.0);
var m = Array(r1,r2,r3);

def readCSVtoDouble(csvfile: String) : Array[Array[Double]] = {
	io.Source.fromFile(csvfile)
    		.getLines()
    		.map(_.split(",").map(_.trim.toDouble))
    		.toArray	 //convert to Array. This entire thing is returned, scala doesnt need you to specify
}


def dot(mat1 : Array[Double], mat2: Array[Double]) : Double = {
		mat1
		.zip(mat2) //zip puts corresponding Array indexes into tuples. Super convenient
		.map{t:(Double,Double) => t._1*t._2} //Map goes inside the tuples set up by .zip and applies the * function
		.reduceLeft(_+_)
}

def matmultiply(mat1 : Array[Array[Double]], mat2: Array[Array[Double]]) : Array[Array[Double]] = {
	var Tmat2 = T(mat2);
	outputM = Array.fill[Array[Double]](mat1.length)(Array.fill[Double](mat.length)(0))
	for(c <- 0 to mat2.length -1){
		for(cc <- 0 to mat1.length){
			outputM(c)(cc) = dot(mat1(c),Tmat2(cc))
		}
	}
}

def T(matrix : Array[Array[Double]]) : Array[Array[Double]] = {
	var cutmatrix = matrix; //Try finding a more scalaish way to do this. Recursive function maybe? Loops don't seem to be the right style
	var outputmatrix = Array.fill[Array[Double]](matrix(0).length)(Array.fill[Double](matrix.length)(0)) //Initialize an output 'matrix'
	for(c <- 0 to outputmatrix.length-1){
		outputmatrix(c) = cutmatrix.map(_.head)
		cutmatrix = cutmatrix.map(_.tail)}
	outputmatrix
}

def sigmoid(x: Double, derivative: Boolean) : Double = {
	if(derivative){Math.exp(x)/((Math.exp(x) + 1)*(Math.exp(x) + 1))}
	else{1/(1+Math.exp(-x))}
}

/*
val rand = scala.util.Random
def NeuralNetTrainer(filename: String, iterations: Integer) : Array[Double] = {

	fulldata = readCSVtoDouble(filename); //Reading in my toy file, generated by my .py script
	targets = fulldata.map{_.reverse}.map{_.head}; //Getting targets, which are the last column of CSV
	features = fulldata.map{_.reverse}.map{_.tail}.map{_.reverse}; //Getting features, which are the first N columns

	// Here I'm generating random weights for my two 'synaptic layers'
	layer1 = Array.fill[Array[Double]](features(0).length)(Array.fill[Double](features.length)(rand.nextInt)); 
	layer2 = Array.fill[Double](targets.length)(rand.nextInt); //This layer will be applied to predictions obtained from layer1

		//Predictions at each layer, feeding forward to the next layers.
		pred1 = matmultiply(features, layer1).map{x=> sigmoid(x,false)}; //Each prediction is fed through the sigmoid function to return a value between 0 and 1
		pred2 = matmultiply(pred1, layer2).map{x=> sigmoid(x,false)}; //These map onto our actual predictions

		//Final layer prediction error
		error2 = targets.zip(pred2).map{t(Double,Double) => t._1 - t._2}; 

		//Back-propagation time!
		d_layer2 = pred2.map{x=> sigmoid(x,true)}.map{x=> x*error2};

		error1 = matmultiply(d_layer2,T(layer1))		
		

*/